\section{Team}

\subsection*{Deliverable 3 - Initial Setup}

This part deals with camera calibration for the drone, specifically obtaining the intrinsic matrix and distortion coefficients. The goal is to transform detected pixel coordinates into undistorted 3D bearing vectors that can be used for geometric vision tasks.

The implementation resides in the function \lstinline{calibrateKeypoints}. It takes two sets of 2D keypoints and converts them into normalized bearing vectors using the camera parameters. The method source is in appendix~\ref{sub:calibratekeypoints}. After undistorting the points with \lstinline{cv::undistortPoints}, each corrected 2D point \((x, y)\) is lifted to a 3D direction vector \((x, y, 1)\). Since the intrinsic matrix has already been accounted for during undistortion, the effective focal length becomes 1, and the resulting vectors are normalized to unit length.

This function is called inside \lstinline{cameraCallback} with a single line:

\begin{minted}{cpp}
calibrateKeypoints(
  pts1, pts2,
  bearing_vector_1,
  bearing_vector_2
);
\end{minted}

Once populated, \lstinline{bearing_vector_1} and \lstinline{bearing_vector_2} are passed to an OpenGV adapter:

\begin{minted}{cpp}
Adapter adapter_mono(
  bearing_vector_1,
  bearing_vector_2
);
\end{minted}

This prepares the data for the RANSAC-based relative pose estimation that follows.

\subsection*{Deliverable 4 - 2D-2D Correspondences}

We implemented and evaluated three geometric algorithms for estimating the relative camera motion between consecutive frames using 2D–2D feature matches.

\subsubsection*{Experimental Methodology}

Feature correspondences were obtained using the SIFT-based tracker from Lab 5. Since all geometric solvers assume a calibrated pinhole camera, raw pixel coordinates were first undistorted using the known intrinsic matrix $K$ and distortion coefficients $D$, then converted into normalized bearing vectors via the \lstinline{calibrateKeypoints} function.

Three pose estimation methods from the OpenGV library were tested. \emph{Nistér’s 5-point algorithm} is a minimal solver for the essential matrix, \emph{Longuet-Higgins’ 8-point algorithm} is a linear least-squares solver, and \emph{2-point algorithm (known rotation)} estimates translation direction only, assuming exact relative rotation—here supplied by ground-truth odometry to mimic a high-precision IMU.

Because monocular vision yields pose estimates up to an unknown scale, the estimated translation vector was normalized and rescaled using the magnitude of the ground-truth translation (via \lstinline{scaleTranslation}) to enable meaningful trajectory visualization in RViz.

\subsubsection*{Implementation Details}

The core logic resides in \lstinline{cameraCallback}. Before invoking any solver, we ensured a sufficient number of tracked features were available. Keypoints were undistorted using \lstinline{cv::undistortPoints} to map them onto the normalized image plane.

Robust estimation was performed using \lstinline{opengv::sac::Ransac}:

For the 5-point and 8-point methods, we used \lstinline{CentralRelativePoseSacProblem}.

For the 2-point method, relative rotation was computed from ground-truth poses (\(R_{\text{GT}} = R_{\text{prev}}^\top R_{\text{curr}}\)), and \lstinline{TranslationOnlySacProblem} was used to estimate translation direction only.

A new boolean parameter \lstinline{use_ransac} and an integer parameter \lstinline{pose_estimator} were added The system is run as fig.~\ref{fig:2d_ransac}

\begin{minted}{console}
roslaunch lab_6 video_tracking.launch pose_estimator:=0 use_ransac:=True
\end{minted}

\subsubsection*{Performance Evaluation}

\paragraph{Impact of RANSAC}

We compared the 5-point algorithm with and without RANSAC to assess the effect of outlier rejection. Results (Fig.~\ref{fig:rpe_comparison}) show:

\emph{Without RANSAC}: large, erratic spikes in both rotation (often exceeding 10°) and translation error, confirming that even a few mismatched features severely degrade minimal solvers.

\emph{With RANSAC}: errors remain low and stable—rotation under 2°, and bounded translation direction error—demonstrating RANSAC’s effectiveness in filtering outliers.

\paragraph{Algorithm Comparison}

Fig.~\ref{fig:algorithm_comparison} compares all three RANSAC-enabled methods:

\emph{Rotation error}: The 2-point method shows near-zero error by design (ground-truth rotation was provided). The 5-point and 8-point methods perform similarly (typically < 3°), with the 5-point method slightly more stable due to its nonlinear essential matrix constraints.

\emph{Translation error}: All methods fluctuate, especially in low-parallax or planar segments—common challenges in monocular VO. The 2-point method maintains the lowest baseline error, confirming that known rotation improves translation estimation, though it remains sensitive to feature noise.

These results illustrate the trade-offs between algorithmic assumptions, available sensor data, and real-world robustness in visual odometry.

\subsection*{Deliverable 5 - 3D-3D Correspondences}

This section estimates the relative camera motion by aligning two sets of 3D points (3D--3D registration), which recovers the trajectory with \emph{absolute scale}—eliminating the need for the ground-truth scaling trick used in Deliverable 4.

\subsubsection*{Experimental Methodology}

We moved beyond monocular constraints by incorporating depth data from the RGB-D sensor.

\paragraph{The workflow is as follows}

For each matched keypoint \((u, v)\) in the RGB image, we read the corresponding depth value \(d\) from the registered depth image. The 2D point was first converted to a normalized bearing vector using the inverse of the intrinsic matrix, then scaled by depth to obtain a 3D point in the camera frame:

\begin{equation*}
P_{\text{cam}} = d \cdot K^{-1} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}.
\end{equation*}

We used \textit{Arun’s method}—a closed-form SVD-based solution—to compute the rigid transformation \((R, t)\) that best aligns the previous frame’s 3D points (\(P_{\text{prev}}\)) to those of the current frame (\(P_{\text{curr}}\)).

To handle noisy depth and mismatched features, the solver was wrapped in a RANSAC loop provided by OpenGV.

\subsubsection*{Implementation Details}

The logic was implemented in \lstinline{cameraCallback}, in case 3.

We looped over matched keypoints, retrieved depth via \lstinline{depth.at<float>(y, x)}, and built two \lstinline{opengv::points_t} point clouds. We used \lstinline{PointCloudAdapter} and \lstinline{PointCloudSacProblem} to interface with the RANSAC solver. The inlier threshold was set in meters (e.g., 0.1\,m) to reject points with large 3D residuals. The \lstinline{scale_translation} parameter was explicitly disabled in the launch file (\lstinline{scale_translation:=0}), ensuring the output translation reflects true physical scale from visual data alone.

A new launch parameter \lstinline{scale_translation} was added. The system is run as:

\begin{minted}{console}
roslaunch lab_6 video_tracking.launch pose_estimator:=3 scale_translation:=0
\end{minted}

\subsubsection*{Performance Evaluation}

We compared the estimated trajectory against ground truth. Results are shown in Fig.~\ref{fig:arun_3d}.

The translation error (top plot) is now in \emph{meters}, not arbitrary units. Errors stay low—typically between \(0.05\) and \(0.2\) meters—confirming that depth data enables true metric-scale estimation. Rotation error (bottom plot) remains consistently below \(1.0^\circ\), showing that 3D--3D alignment provides a strong geometric constraint on orientation, often surpassing 2D--2D methods that suffer from degeneracies. The trajectory shows no scale drift, a common failure mode in monocular VO. This highlights the advantage of RGB-D sensing for reliable indoor navigation.

\subsection*{Evaluation on Drone Racing Dataset}

\subsubsection*{Data Acquisition}

To test robustness under more demanding conditions, we recorded a custom dataset—\lstinline{vnav-lab4-group31.bag}—using a drone racing simulator. This sequence includes high-speed maneuvers, sharp turns, and rapid accelerations, making it significantly more challenging than the relatively calm \lstinline{office.bag}.

\subsubsection*{Running Snapshot}

Fig.~\ref{fig:running_result} shows a typical run on this dataset.

\subsubsection*{Performance Analysis}

We first ran the monocular pipelines (5-point, 8-point, and 2-point) on the racing data. Results are shown in Figure~\ref{fig:algorithm_comparison_lab4}.

The aggressive motion exposes clear limitations of classical geometric VO. The 2-point method (orange) shows zero rotation error, as expected—ground-truth rotation was provided. In contrast, the 8-point method (green) fails dramatically, with rotation errors spiking to nearly \(200^\circ\) in several frames. The 5-point method (blue) is more stable but still far noisier than in the office environment. All three methods suffer from large and erratic translation errors. This is largely due to motion blur and rapid viewpoint changes, which degrade SIFT matching quality. Poor correspondences directly undermine the geometric solvers, regardless of algorithmic sophistication. We then evaluated Arun’s 3D--3D registration method on the same sequence (Figure~\ref{fig:arun_3d_lab4}).

Despite having access to depth and recovering absolute scale, Arun’s method still struggles. Periods of smooth motion yield low errors, confirming the method works under favorable conditions. However, frequent large spikes—translation errors exceeding \(10\) meters and rotation errors surging—reveal a fundamental bottleneck: feature tracking. Since 3D point correspondences are derived from the same 2D matches used in monocular VO, failures in feature detection or matching (due to blur, occlusion, or low texture) propagate directly into 3D registration. This shows that while RGB-D resolves scale ambiguity, it does not eliminate sensitivity to tracking failures in high-dynamics scenarios.

In summary, even with depth or known rotation, visual odometry remains fragile when feature quality degrades—highlighting the need for either tighter sensor fusion (e.g., with IMU) or more robust front-end features for high-speed flight.
