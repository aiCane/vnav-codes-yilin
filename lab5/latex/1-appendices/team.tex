\section{Team}

\subsection{Deliverable - Feature Descriptors (SIFT)}

We provide you with skeleton code for the base class FeatureTracker that provides an abstraction layer for all feature trackers. Furthermore, we give you two empty structures for the SIFT and SURF methods that derive from the class FeatureTracker.

Inside the \texttt{lab5} folder, we provide you with two images `box.png' and `box\_in\_scene.png' (inside the \texttt{images} folder).

We will first ask you to extract keypoints from both images using SIFT. For that, we refer you to the skeleton code in the \texttt{src} folder named \lstinline{track_features.cpp}. Follow the instructions written in the comments; specifically, you will need to complete:

\begin{itemize}
  \item The stub \lstinline{SiftFeatureTracker::detectKeypoints()} and \lstinline{SiftFeatureTracker::describeKeypoints()} in \lstinline{lab5/feature_tracking/src/sift_feature_tracker.cpp}
  \item The first part of \lstinline{FeatureTracker::trackFeatures()} in \lstinline{lab5/feature_tracking/src/feature_tracker.cpp}
\end{itemize}

Once you have implemented SIFT, you can test it by running:

\begin{minted}{console}
roslaunch lab_5 two_frames_tracking.launch descriptor:=SIFT # note you can change the descriptor later
\end{minted}

Your code should be able to plot a figure like the one below (keypoints you detected should not necessarily coincide with the ones in the figure):

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/keypoints}
\end{figure}

Now that we have detected keypoints in each image, we need to find a way to uniquely identify these to subsequently match features from frame to frame. Feature descriptors in the literature are multiple, and while we will not review all of them, they all rely on a similar principle: using the pixel intensities around the detected keypoint to describe the feature.

Descriptors are multidimensional and are typically represented by an array.

Follow the skeleton code in \texttt{src} to compute the descriptors for all the extracted keypoints.

{\small \color{gray} Hint: In OpenCV, SIFT and the other descriptors we will look at are children of the ``Feature2D'' class. We provide you with a SIFT \texttt{detector} object, so look at the \href{https://docs.opencv.org/3.4/d0/d13/classcv_1_1Feature2D.html}{Feature2D ``Public Member Functions''} documentation to determine which command you need to detect keypoints and get their descriptors. }

\subsection{Deliverable - Descriptor-based Feature Matching}

With the pairs of keypoint detections and their respective descriptors, we are ready to start matching keypoints between two images. It is possible to match keypoints just by using a brute force approach. Nevertheless, since descriptors can have high-dimensionality, it is advised to use faster techniques.

In this exercise, we will ask you to use the FLANN (Fast Approximate Nearest Neighbor Search Library), which provides instead a fast implementation for finding the nearest neighbor. This will be useful for the rest of the problem set, when we will use the code in video sequences.

\begin{enumerate}
  \item What is the dimension of the SIFT descriptors you computed?
  \item Compute and plot the matches that you found from the \textit{box.png} image to the \textit{box\_in\_scene.png}.
  \item You might notice that naively matching features results in a significant amount of false positives (outliers). There are different techniques to minimize this issue. The one proposed by the authors of SIFT was to calculate the best two matches for a given descriptor and calculate the ratio between their distances: \lstinline{Match1.distance < 0.8 * Match2.distance}. This ensures that we do not consider descriptors that have been ambiguously matched to multiple descriptors in the target image. Here we used the threshold value that the SIFT authors proposed (0.8).
  \item Compute and plot the matches that you found from the \textit{box.png} image to the \textit{box\_in\_scene.png} after applying the filter that we just described. You should notice a significant reduction of outliers.
\end{enumerate}

Specifically, you will need to complete:

\begin{itemize}
  \item The stub \lstinline{SiftFeatureTracker::matchDescriptors()} in \lstinline{feature_tracking/src/sift_feature_tracker.cpp}
  \item The second part of \lstinline{FeatureTracker::trackFeatures()} in \lstinline{feature_tracking/src/feature_tracker.cpp}

  {\small \color{gray}
    Hint: Note that the \texttt{matches} object is a pointer type, so to use it you will usually have to type \texttt{*matches}. Once you’ve used the FLANN matcher to get the matches, if you want to iterate over all of them you could use a loop like:

    \begin{minted}{cpp}
for (auto& match : *matches) {
// check match.size(), match[0].distance, match[1].distance, etc.
}
    \end{minted}

    Likewise, \texttt{good\_matches} is a pointer so to add to it you will need \lstinline{good_matches->push_back(...)}.
  }
\end{itemize}

\subsection{Deliverable - Keypoint Matching Quality}

Excellent! Now, that we have the matches between keypoints in both images, we can apply many cool algorithms that you will see in subsequent lectures.

For now, let us just use a blackbox function which, given the keypoints correspondences from image to image, is capable of deciding whether some matches are considered outliers.

\begin{enumerate}
  \item Using the function we gave you, compute and plot the inlier and outlier matches, such as in the following figure:

  \begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/kp_matches}
  \end{figure}

  {\small \color{gray}
    Hint: Note that \lstinline{FeatureTracker::inlierMaskComputation} computes an inlier mask of type \lstinline{std::vector<uchar>}, but for \lstinline{cv::drawMatches} you will need a \lstinline{std::vector<char>}. You can go from one to the other by using:

    \lstinline!std::vector<char> char_inlier_mask{inlier_mask.begin(), inlier_mask.end()};!
    % {\ttfamily std::vector<char> char\_inlier\_mask{inlier\_mask.begin(), inlier\_mask.end()};}

    Hint: You will need to call the \lstinline{cv::drawMatches} function twice, first to plot the everything in red (using \lstinline{cv::Scalar(0,0,255}) as the color), and then again to plot inliers in green (using the inlier mask and \lstinline{cv::Scalar(0,255,0)}). The second time, you will need to use the \lstinline{DrawMatchesFlags::DRAW_OVER_OUTIMG} flag to draw on top of the first output. To combine flags for the \lstinline{cv::drawMatches} function, use the bitwise-or operator:

    \lstinline{DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS | DrawMatchesFlags::DRAW_OVER_OUTIMG}
    % {\ttfamily DrawMatchesFlags::NOT\_DRAW\_SINGLE\_POINTS | DrawMatchesFlags::DRAW\_OVER\_OUTIMG}
  }

  \item Now, we can calculate useful statistics to compare feature tracking algorithms. First, let’s compute statistics for SIFT. The code for computing the statistics is already included in the skeleton file. You just need to use the appropriate functions to print them out. Submit a table similar to the following for SIFT (you might not get the same results, but they should be fairly similar):

  TODO: This is a table.
\end{enumerate}

\subsection{Deliverable - Comparing Feature Matching Algorithms on Real Data}
\label{sub:a_deliverable_comparing_feature_matching_algorithms_on_real_data}

The most common algorithms for feature matching use different detection, description, and matching techniques. We’ll now try different techniques and see how they compare against one another:

\subsubsection{Pair of frames}
\label{ssub:a_pair_of_frames}

Fill the previous table with results for other feature tracking algorithms. We ask that you complete the table using the following additional algorithms:

\begin{enumerate}
  \item \href{https://docs.opencv.org/3.4.2/df/dd2/tutorial_py_surf_intro.html}{SURF} (a faster version of SIFT)
  \item \href{https://docs.opencv.org/3.4.2/d1/d89/tutorial_py_orb.html}{ORB} (we will use it for SLAM later!)
  \item Optional : \href{https://docs.opencv.org/3.4.2/df/d0c/tutorial_py_fast.html}{FAST} (detector) + \href{https://docs.opencv.org/3.4.2/dc/d7d/tutorial_py_brief.html}{BRIEF} (descriptor)

  {\small \color{gray} Hint: The SURF functions can be implemented exactly the same as SIFT, while ORB and FAST can also be implemented exactly the same way except that the FLANN matcher must be initialized with a new parameter like this: \lstinline{FlannBasedMatcher matcher(new flann::LshIndexParams(20, 10, 2));}. This is not necessarily the best solution for ORB and FAST however, so we encourage you to look into other methods (e.g. the \href{https://docs.opencv.org/3.4.2/dc/dc3/tutorial_py_matcher.html}{Brute-Force matcher} instead of the FLANN matcher) if you have time. Note for BRIEF, you will need to create a \href{https://docs.opencv.org/3.4.20/d1/d93/classcv_1_1xfeatures2d_1_1BriefDescriptorExtractor.html}{BriefDescriptorExtractor} to create keypoint descriptors when implementing FAST+BRIEF.}
\end{enumerate}

We have provided method stubs in the corresponding header files for you to implement in the CPP files. Please refer to the OpenCV documentation, tutorials, and C++ API when filling them in. You are encouraged to modify the default parameters used by the features extractors and matchers. A complete answer to this deliverable should include a brief discussion of what you tried and what worked the best.

By now, you should have four algorithms, with their pros and cons, capable of tracking features between frames.

{\small \color{gray} Hint: It is normal for some descriptors to perform worse than others, especially on this pair of images – in fact, some may do very poorly, so don’t worry if you observe this.}

\textbf{We got the terminal logs to be}:

\begin{minted}{text}
Avg. Keypoints 1 Size: 603
Avg. Keypoints 2 Size: 969
Avg. Number of matches: 603
Avg. Number of good matches: 98
Avg. Number of Inliers: 77
Avg. Inliers ratio: 0.785714
Num. of samples: 1
```,
caption: [SIFT output],
kind: "code",
supplement: [Log],
\end{minted}

\begin{minted}{text}
Avg. Keypoints 1 Size: 997
Avg. Keypoints 2 Size: 1822
Avg. Number of matches: 997
Avg. Number of good matches: 135
Avg. Number of Inliers: 90
Avg. Inliers ratio: 0.666667
Num. of samples: 1
\end{minted}

\begin{minted}{text}
Avg. Keypoints 1 Size: 500
Avg. Keypoints 2 Size: 500
Avg. Number of matches: 500
Avg. Number of good matches: 8
Avg. Number of Inliers: 7
Avg. Inliers ratio: 0.875
Num. of samples: 1
```,
caption: [ORB output],
kind: "code",
supplement: [Log],
\end{minted}

\begin{minted}{text}
Avg. Keypoints 1 Size: 2266
Avg. Keypoints 2 Size: 3653
Avg. Number of matches: 2266
Avg. Number of good matches: 41
Avg. Number of Inliers: 17
Avg. Inliers ratio: 0.414634
Num. of samples: 1
```,
caption: [FAST+BRIEF output],
kind: "code",
supplement: [Log],
\end{minted}

\subsubsection{Real Datasets}
\label{ssub:a_real_datasets}

Let us now use an actual video sequence to track features from frame to frame and push these algorithms to their limit!

We have provided you with a set of datasets in rosbag format \href{https://github.com/MIT-SPARK/VNAV-lab-data/tree/master}{here}. Please download the following datasets, which are the two ``easiest'' ones:

\begin{itemize}
  \item \texttt{30fps\_424x240\_2018-10-01-18-35-06.bag}
  \item \texttt{vnav-lab5-smooth-trajectory.bag}
\end{itemize}

Testing other datasets may help you to identify the relative strengths and weaknesses of the descriptors, but this is not required.

We also provide you with a roslaunch file that executes two ROS nodes:

\begin{minted}{console}
roslaunch lab_5 video_tracking.launch path_to_dataset:=/home/$USER/Downloads/<NAME_OF_DOWNLOADED_FILE>.bag
\end{minted}

\begin{itemize}
  \item One node plays the rosbag for the dataset
  \item The other node subscribes to the image stream and is meant to compute the statistics to fill the table below
\end{itemize}

You will need to first specify in the launch file the path to your downloaded dataset. Once you’re done, you should get something like this:

{\small \color{gray} Hint: You may need to change your plotting code in \lstinline{FeatureTracker::trackFeatures} to call \lstinline{cv::waitKey(10)} instead of \lstinline{cv::waitKey(0)} after imshow in order to get the video to play continuously instead of frame-by-frame on every keypress.}

\textbf{The logs of Real Datasets are}:

\begin{minted}{text}
Avg. Keypoints 1 Size: 321.819
Avg. Keypoints 2 Size: 321.602
Avg. Number of matches: 321.819
Avg. Number of good matches: 160.974
Avg. Number of Inliers: 149.751
Avg. Inliers ratio: 0.928412
Num. of samples: 425
```,
caption: [SIFT output],
kind: "code",
supplement: [Log],
\end{minted}

\begin{minted}{text}
Avg. Keypoints 1 Size: 389.408
Avg. Keypoints 2 Size: 389.252
Avg. Number of matches: 389.408
Avg. Number of good matches: 234.594
Avg. Number of Inliers: 216.93
Avg. Inliers ratio: 0.926189
Num. of samples: 488
```,
caption: [SURF output],
kind: "code",
supplement: [Log],
\end{minted}

\begin{minted}{text}
Avg. Keypoints 1 Size: 268.751
Avg. Keypoints 2 Size: 268.612
Avg. Number of matches: 268.751
Avg. Number of good matches: 166.031
Avg. Number of Inliers: 160.54
Avg. Inliers ratio: 0.964453
Num. of samples: 541
```,
caption: [ORB output],
kind: "code",
supplement: [Log],
\end{minted}

\begin{minted}{text}
Avg. Keypoints 1 Size: 741.136
Avg. Keypoints 2 Size: 741.256
Avg. Number of matches: 741.136
Avg. Number of good matches: 499.004
Avg. Number of Inliers: 465.635
Avg. Inliers ratio: 0.928044
Num. of samples: 477
```,
caption: [FAST+BRIEF output],
kind: "code",
supplement: [Log],
\end{minted}

Finally, we ask you to summarize your results in one table for each dataset and asnwer some questions:

\begin{itemize}
  \item Compute the \textbf{average} (over the images in each dataset) of the statistics on the table below for the different datasets and approaches. You are free to use whatever parameters you find result in the largest number of inliers.

  TODO: this is a table

  \item What conclusions can you draw about the capabilities of the different approaches? Please make reference to what you have tested and observed.

  {\small \color{gray}
    Hint: \textbf{We don’t expect long answers, there aren’t specific answers we are looking for, and you don’t need to answer every suggested question below!} We are just looking for a few sentences that point out the main differences you noticed and that are supported by your table/plots/observations.

    Some example questions to consider:

    Which descriptors result in more/fewer keypoints?

    How do they the descriptors differ in ratios of good matches and inliers?

    Are some feature extractors or matchers \href{https://stackoverflow.com/a/22387757}{faster} than others?

    What applications are they each best suited for? (e.g. when does speed vs quality matter)
  }
\end{itemize}

\begin{minted}{text}
Avg. Keypoints 1 Size: 137.468
Avg. Keypoints 2 Size: 136.608
Avg. Number of matches: 137.468
Avg. Number of good matches: 67.9915
Avg. Number of Inliers: 50.8766
Avg. Inliers ratio: 0.75093
Num. of samples: 235
```,
caption: [SIFT output],
kind: "code",
supplement: [Log],
\end{minted}

\begin{minted}{text}
Avg. Keypoints 1 Size: 1009.98
Avg. Keypoints 2 Size: 1006.7
Avg. Number of matches: 1009.98
Avg. Number of good matches: 457.648
Avg. Number of Inliers: 345.381
Avg. Inliers ratio: 0.778896
Num. of samples: 310
```,
caption: [SURF output],
kind: "code",
supplement: [Log],
\end{minted}

\begin{minted}{text}
Avg. Keypoints 1 Size: 258.294
Avg. Keypoints 2 Size: 257.972
Avg. Number of matches: 258.294
Avg. Number of good matches: 133.933
Avg. Number of Inliers: 116.456
Avg. Inliers ratio: 0.878598
Num. of samples: 465
```,
caption: [ORB output],
kind: "code",
supplement: [Log],
\end{minted}

\begin{minted}{text}
Avg. Keypoints 1 Size: 524.664
Avg. Keypoints 2 Size: 523.742
Avg. Number of matches: 524.664
Avg. Number of good matches: 313.428
Avg. Number of Inliers: 273.366
Avg. Inliers ratio: 0.877645
Num. of samples: 465
```,
caption: [FAST+BRIEF output],
kind: "code",
supplement: [Log],
\end{minted}

\subsection{Deliverable - Feature Tracking: Lucas Kanade Tracker}

So far we have worked with descriptor-based matching approaches. As you have seen, these approaches match features by simply comparing their descriptors. Alternatively, feature tracking methods use the fact that, when recording a video sequence, a feature will not move much from frame to frame. We will now use the most well-known differential feature tracker, also known as Lucas-Kanade (LK) Tracker.

\begin{enumerate}
  \item Using \href{https://docs.opencv.org/3.3.1/d7/d8b/tutorial_py_lucas_kanade.html}{OpenCV’s documentation} and the \href{https://docs.opencv.org/3.3.1/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323}{C++ API for the LK tracker}, track features for the video sequences we provided you by using the \href{https://docs.opencv.org/3.4.2/dc/d0d/tutorial_py_features_harris.html}{Harris corner detector} (like \href{https://www.dropbox.com/s/zodssejrdl9vqdb/jpl_cave.mp4?dl%3D0}{here}). Show the feature tracks at a given frame extracted when using the Harris corners, such as this:

  \begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/lk_tracker}
  \end{figure}

  \item Add an extra entry to the table used in \textbf{Deliverable 6} \ref{sub:a_deliverable_comparing_feature_matching_algorithms_on_real_data}. using the Harris + LK tracker that you implemented.
  \item What assumption about the features does the LK tracker rely on?
  \item Comment on the different results you observe between the table in this section and the one you computed in the other sections.

  {\small \color{gray} Hint: You will need to convert the image to grayscale with \lstinline{cv::cvtColor} and will want to look into the documentation for \lstinline{cv::goodFeaturesToTrack} and \lstinline{cv::calcOpticalFlowPyrLK}. The rest of the trackFeatures() function should be mostly familiar feature matching and inlier mask computation similar to the previous sections. Also note that the \texttt{status} vector from calcOpticalFlowPyrLK indicates the matches.}

  {\small \color{gray} Hint: For the show() method, you will just need to create a copy of the input frame and then make a loop that calls \lstinline{cv::line} and \lstinline{cv::circle} with correct arguments before calling \texttt{imshow}.}
\end{enumerate}

\subsection{[Optional] Deliverable - Optical Flow}

LK tracker estimates the optical flow for sparse points in the image. Alternatively, dense approaches try to estimate the optical flow for the whole image. Try to calculate your \href{https://www.dropbox.com/s/37u2b5xax6puf5j/own_flow.mp4?dl=0}{own optical flow}, or the flow of a video of your choice, using \href{https://docs.opencv.org/3.3.1/dc/d6b/group__video__track.html#ga5d10ebbd59fe09c5f650289ec0ece5af}{Farneback’s algorithm}.

{\small \color{gray} Hint: Take a look at this \href{https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html}{tutorial}, specifically the section on dense optical flow. Please post on piazza if you run into any issues or get stuck anywhere.}
